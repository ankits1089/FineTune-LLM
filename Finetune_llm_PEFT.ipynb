{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNo2zw2QQt4YibQAszO/qHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankits1089/FineTune-LLM/blob/main/Finetune_llm_PEFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #install PEFT & datasets\n",
        "# !pip install datasets\n",
        "# !pip install -U peft"
      ],
      "metadata": {
        "id": "hyCGB85mmqpp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ErE6N9639Rnr"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # all output p\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically detect device (GPU > MPS > CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbkCe_4nL5BZ",
        "outputId": "66233003-5082-41cb-a2c7-96ac46104010"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the model and tokenizer\n",
        "model_name = \"facebook/opt-350m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Load model & move to device\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "HOIk0w1Q9VsB"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize prompt\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer(\"What is AI?\", return_tensors=\"pt\",padding=True, truncation=True,max_length=100)"
      ],
      "metadata": {
        "id": "3MzruczldgMP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation\n",
        "outputs = model.generate(**inputs.to(device))\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szKwhWrI9kWb",
        "outputId": "8b5b680e-97dd-4b09-b3dd-a97419edf956"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is AI?\n",
            "\n",
            "AI is a computer program that is able to learn and adapt to the environment. It is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of paramters\n",
        "model.num_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKqVD0SiQKBi",
        "outputId": "25324d41-e49d-4971-d30a-a4c754ba6ea7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "331196416"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kibBQrad-dSu",
        "outputId": "78fec5ac-d14f-4466-8e97-0cee2803a4e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/My Drive/Data Science/Projects"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91sCeSeM_Epe",
        "outputId": "d3010116-28ee-408b-8e41-4253379e0dc9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Data Science/Projects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a question and answer set in a list with dictionary\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "QA_list = []\n",
        "\n",
        "with open ('QA.txt') as doc:\n",
        "    Q_counter = 0\n",
        "    for line in doc:\n",
        "        if 'Question' in line:\n",
        "            Q_dict = {'question':line.replace('Question:\"',\"\").strip().strip('\"'),'answer':''}\n",
        "            QA_list.append(Q_dict)\n",
        "            Q_counter+=1\n",
        "        elif 'Answer' in line:\n",
        "            A_counter = Q_counter-1\n",
        "            modified_line = line.replace('Answer:\"',\"\").strip().strip('\"')\n",
        "            QA_list[A_counter]['answer'] = modified_line\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "dataset = Dataset.from_list(QA_list)"
      ],
      "metadata": {
        "id": "t3Bk0Y_O9kKd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check dataest\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L_gU1FybkSr",
        "outputId": "10e57f4e-2433-41d1-eee0-acd2c536e116"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer'],\n",
              "    num_rows: 77\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize as question answer pair which will be used for training\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    text = examples[\"question\"] + examples[\"answer\"]\n",
        "\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    tokenizer.truncation_side = \"right\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        # return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        # padding=True,\n",
        "        truncation=True,\n",
        "        max_length=30\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "0XSdBbhD_i7j"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=False,\n",
        "    # batch_size=1,\n",
        "    # drop_last_batch=True\n",
        ")\n",
        "\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8TszWqrGzGd",
        "outputId": "69d5a234-74e0-41e6-d2be-1033d02e98e3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 77\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding labels\n",
        "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
      ],
      "metadata": {
        "id": "qgRQR7g-G3vo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare train test split\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=123)\n",
        "print(split_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-mulsWfG3ny",
        "outputId": "c1e4f94a-44d0-4b1b-d0ff-c4099d06d82f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 69\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 8\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=512, max_output_tokens=30):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  # device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "      **input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "kx7hlJJlG3cn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = split_dataset['test'][3]['question']\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(f\"Correct answer from Lamini docs: {split_dataset['test'][3]['answer']}\")\n",
        "print(\"\\nModel's answer: \")\n",
        "print(inference(test_text, model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dx_ARCiHTN8",
        "outputId": "c2f47917-b1b5-4291-df49-f88d27bcd6bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): Why is insisting that you are 'right' and the other person is 'wrong' considered bad communication?\n",
            "Correct answer from Lamini docs: Because it creates conflict and prevents understanding between both parties.\n",
            "\n",
            "Model's answer: \n",
            "\n",
            "Because it's not a matter of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create lora config\n",
        "\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "# create LoRA configuration object\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, # type of task to train on\n",
        "    inference_mode=False, # set to False for training\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    r=8, # dimension of the smaller matrices\n",
        "    lora_alpha=32, # scaling factor\n",
        "    lora_dropout=0.1, # dropout of LoRA layers\n",
        "    modules_to_save=[\"lm_head\"]\n",
        ")"
      ],
      "metadata": {
        "id": "7gfih5RAopc8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.unload()\n",
        "# model.delete_adapter('default')"
      ],
      "metadata": {
        "id": "1u270G2UqHZS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using data collator for batch processing\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,padding=True)"
      ],
      "metadata": {
        "id": "NNhK304T6kfe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters() # The number of parameters is high as lm head is being also trained.\n",
        "# Though the result is coming to be similar without lm_head for similar number of steps (considerably less paramters to be trained: 700K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHbyRLRCrZCn",
        "outputId": "3b128aa2-daa8-415a-8641-57719d11134c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 26,525,696 || all params: 357,722,112 || trainable%: 7.4152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model config\n",
        "model.peft_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm1fhRoi0q9P",
        "outputId": "7813220a-90ee-4a7c-b4b3-a5622d8dfb7c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'default': LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='facebook/opt-350m', revision=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# initialising training arguments\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-6,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=12,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=-1,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=4,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir='fine_tuned_opt_350m_final',\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=4, # Number of update steps between two evaluations\n",
        "  save_steps=16, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=4, # Batch size for evaluation\n",
        "  eval_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  # fp16=False,\n",
        "  logging_steps=8,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 1,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=2,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")"
      ],
      "metadata": {
        "id": "jrzteTeIHTF4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=split_dataset[\"train\"],\n",
        "    eval_dataset=split_dataset[\"test\"],\n",
        "    data_collator=data_collator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21J9JekUHS6q",
        "outputId": "99077a20-95f7-4ab6-cf46-fa3f38264399"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# trainer.train()\n",
        "trainer.train(resume_from_checkpoint=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "LCqB1Mv75e6s",
        "outputId": "92d7cb2b-3a19-4279-e6c1-c441eaeb845a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 00:09, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.186000</td>\n",
              "      <td>2.065768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.186000</td>\n",
              "      <td>2.067105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.197600</td>\n",
              "      <td>2.075464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.197600</td>\n",
              "      <td>2.082425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.204800</td>\n",
              "      <td>2.093981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.204800</td>\n",
              "      <td>2.098860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.184400</td>\n",
              "      <td>2.099129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.184400</td>\n",
              "      <td>2.100057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.193000</td>\n",
              "      <td>2.102139</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=216, training_loss=0.03232844643018864, metrics={'train_runtime': 9.4661, 'train_samples_per_second': 87.47, 'train_steps_per_second': 22.818, 'total_flos': 49165858897920.0, 'train_loss': 0.03232844643018864, 'epoch': 12.0})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # load the finetuned model\n",
        "\n",
        "# from peft import PeftModel, PeftConfig\n",
        "# from transformers import AutoModelForCausalLM\n",
        "\n",
        "# config = PeftConfig.from_pretrained(\"fine_tuned_opt_350m_final/checkpoint-144\")\n",
        "# model_base = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "# model_finetuned = PeftModel.from_pretrained(model_base,\n",
        "# \"fine_tuned_opt_350m_final/checkpoint-144\",\n",
        "#  is_trainable=True\n",
        "# )"
      ],
      "metadata": {
        "id": "9qJRclc8vg-R"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the model output on the question - what is the larget lake?"
      ],
      "metadata": {
        "id": "d3LSyKy2wka6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize prompt\n",
        "question = \"What is the largest lake?\"\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer(question, return_tensors=\"pt\",padding=True, truncation=True,max_length=100)\n"
      ],
      "metadata": {
        "id": "TPcKMVaWc25x"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation\n",
        "outputs = model.generate(**inputs.to(device))\n",
        "print(\"Question: \",question,\"\\nAnswer:\",tokenizer.decode(outputs[0], skip_special_tokens=True)[len(question):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnpTv7c4rF8H",
        "outputId": "9e9c4032-9742-44fb-ca4a-314a8fb11668"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What is the largest lake? \n",
            "Answer: Please let's keep our discussion related to good or bad communication.Please let's keep our discussion related\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the result with that of the base model"
      ],
      "metadata": {
        "id": "2iOe9I2yw7B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/opt-350m\"\n",
        "# Load model & move to device\n",
        "model_base = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "iHlOhMkCzWXH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation\n",
        "outputs = model_base.generate(**inputs.to(device))\n",
        "print(\"Question: \",question,\"\\nAnswer:\",tokenizer.decode(outputs[0], skip_special_tokens=True)[len(question):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtTNAYXDzKch",
        "outputId": "58e8af6e-ab54-49da-c65a-ba8384dde6c8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What is the largest lake? \n",
            "Answer: \n",
            "Lake of the Gods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained model output from QA document vs base model"
      ],
      "metadata": {
        "id": "08zjRcWzxVFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mention question number\n",
        "qno = 35\n",
        "\n",
        "test_text = split_dataset['train'][qno]['question']\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(f\"\\nCorrect answer from Lamini docs: {split_dataset['train'][qno]['answer']}\")\n",
        "print(\"\\nModel's answer: \")\n",
        "print(inference(test_text, model, tokenizer))\n",
        "print(\"\\nBase model's answer: \")\n",
        "print(inference(test_text, model_base, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGC9gL8mqRLZ",
        "outputId": "29030ff9-e2a0-42e4-99f0-75bd36ffeac9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): What is feeling empathy?\n",
            "\n",
            "Correct answer from Lamini docs: Acknowledging how the other person is feeling based on what they have expressed.\n",
            "\n",
            "Model's answer: \n",
            "It helps you understand the other personâ€™s point of view.It helps you understand their point of view.It\n",
            "\n",
            "Base model's answer: \n",
            "\n",
            "\n",
            "What is feeling empathy?\n",
            "\n",
            "What is feeling empathy?\n",
            "\n",
            "What is feeling empathy?\n",
            "\n",
            "What\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_aaspeSov6C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}